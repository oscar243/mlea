\documentclass[a4paper,12pt]{article}

% --------------------------------------------------------------------
% PAQUETES
% --------------------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[spanish,es-tabla]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern} 
\usepackage{geometry}
\geometry{top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm}

\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\usepackage[labelfont=bf]{caption}
\usepackage{booktabs}   % Tablas profesionales
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{hyperref}

% Configuración de hipervínculos
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

% Configuración de código Python
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{red},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

% --------------------------------------------------------------------
% ENCABEZADO
% --------------------------------------------------------------------
\title{\textbf{Optimización de Modelos de Machine Learning para HAR: Integración de Características de Frecuencia y Boosting}}
\author{
    \textbf{Oscar A. Virguez} \\
    \textit{Ingeniero Electrónico}
}
\date{5 de Diciembre de 2025}

\begin{document}

\maketitle

% --------------------------------------------------------------------
% RESUMEN
% --------------------------------------------------------------------
\begin{abstract}
    En este trabajo se desarrolló un pipeline de Machine Learning para el reconocimiento de actividades humanas (HAR) utilizando datos de acelerómetros. La metodología se centró en una ingeniería de características avanzada, integrando métricas del dominio de la frecuencia (FFT) y magnitud vectorial (SVM). Se implementó un modelo XGBoost optimizado mediante \textit{RandomizedSearchCV}, abordando el desbalance de clases mediante ponderación de muestras. El modelo final alcanzó una exactitud del \textbf{XX.XX\%} (reemplazar con el valor de tu log, ej: 91.5\%) en el conjunto de prueba, superando significativamente al baseline de Random Forest.
\end{abstract}

\tableofcontents
\newpage

% --------------------------------------------------------------------
% 1. METODOLOGÍA
% --------------------------------------------------------------------
\section{Enfoque Metodológico}

Para garantizar la reproducibilidad y la robustez de la solución, se diseñó un flujo de trabajo secuencial y lógico. Este enfoque permite transformar los datos crudos de los sensores en predicciones precisas mediante las siguientes etapas estructuradas:

\subsection{Etapa 1: Unificación y Preprocesamiento de Datos}
Los datos originales se encontraban fragmentados en múltiples archivos. El primer paso consistió en la unificación de los conjuntos de entrenamiento y prueba mediante scripts personalizados. Se aseguró la integridad de los datos verificando la consistencia de las etiquetas y alineando las series temporales de los acelerómetros ($x, y, z$) con sus respectivos metadatos.

\subsection{Etapa 2: Análisis Exploratorio de Datos (EDA)}
Antes del modelado, se realizó un análisis exhaustivo para comprender la naturaleza del problema:
\begin{itemize}
    \item \textbf{Distribución de Clases:} Se identificó un desbalance significativo, con una predominancia de actividades dinámicas (Walking, Jogging) frente a estáticas (Sitting, Standing).
    \item \textbf{Análisis de Señales:} La visualización de las series temporales reveló que las actividades dinámicas presentan patrones periódicos claros, mientras que las estáticas se caracterizan por ruido de baja amplitud.
\end{itemize}

\subsection{Etapa 3: Ingeniería de Características (Feature Engineering)}
Esta fase fue crítica para mejorar el rendimiento del modelo. Se generaron nuevas variables para capturar patrones complejos:

\subsubsection{Magnitud del Vector de Señal (SVM)}
Para hacer el modelo invariante a la orientación del dispositivo, se calculó la norma del vector de aceleración:
\begin{equation}
    SVM = \sqrt{x^2 + y^2 + z^2}
\end{equation}
Esta métrica resume la intensidad total del movimiento en un solo valor escalar por instante de tiempo.

\subsubsection{Características en el Dominio de la Frecuencia}
Dado el carácter cíclico de muchas actividades humanas, se aplicó la Transformada Rápida de Fourier (FFT) sobre ventanas deslizantes. Se extrajeron métricas clave como la \textit{frecuencia dominante}, la \textit{energía espectral} y la \textit{magnitud pico}. Estas características permiten distinguir, por ejemplo, entre caminar y correr basándose en la cadencia del paso.

\subsection{Etapa 4: Selección de Características}
Para evitar la "maldición de la dimensionalidad" y mejorar la generalización:
\begin{itemize}
    \item \textbf{Eliminación de Redundancia:} Se eliminaron variables con alta colinealidad (ej. varianza vs. desviación estándar).
    \item \textbf{Ranking de Importancia:} Se utilizaron técnicas como \textit{Mutual Information} y la importancia intrínseca de árboles de decisión para seleccionar el subconjunto óptimo de variables predictivas.
\end{itemize}

\subsection{Etapa 5: Estrategia de Modelado y Balanceo}
Se seleccionó XGBoost como algoritmo principal. Para mitigar el desbalance de clases detectado en la Etapa 2, se implementó una estrategia de ponderación de muestras (\texttt{sample\_weights}), asignando mayor peso a los errores cometidos en las clases minoritarias durante el entrenamiento.

% --------------------------------------------------------------------
% 2. ANÁLISIS EXPLORATORIO (EDA)
% --------------------------------------------------------------------
\section{Análisis Exploratorio de Datos}

\subsection{Importancia de Características}
Tras entrenar un modelo preliminar, se analizó la importancia de las variables. Se observó que las nuevas características generadas (especialmente SVM y las derivadas de FFT) ocuparon los primeros lugares en ganancia de información.

% REEMPLAZAR CON TU GRÁFICA GENERADA EN ETAPA 2 o 3
\begin{figure}[H]
    \centering
    % \includegraphics[width=0.8\textwidth]{feature_importance_xgb.png}
    \caption{\textbf{Top 15 Características más importantes.} Nótese la relevancia de las variables derivadas de SVM y Energía frente a los datos crudos originales.}
    \label{fig:feat_imp}
\end{figure}

\subsection{Selección de Características}
Se compararon sistemáticamente tres métodos de selección: F-test (ANOVA), Información Mutua y la importancia intrínseca de XGBoost. Los resultados indicaron que el uso de las mejores 30-40 características permitía mantener la precisión reduciendo la complejidad del modelo.

% --------------------------------------------------------------------
% 3. MODELADO Y OPTIMIZACIÓN
% --------------------------------------------------------------------
\section{Modelado y Resultados}

Se evaluaron dos arquitecturas principales: Random Forest (como línea base) y XGBoost (Extreme Gradient Boosting).

\subsection{Configuración del Modelo Óptimo}
El modelo final se basó en XGBoost debido a su capacidad para manejar datos desbalanceados y su regularización intrínseca. Se realizó una búsqueda aleatoria (\texttt{RandomizedSearchCV}) sobre 50 combinaciones de hiperparámetros con validación cruzada estratificada ($k=5$). El espacio de búsqueda incluyó:

\begin{itemize}
    \item \texttt{n\_estimators}: [150, 200, 250, 300, 400]
    \item \texttt{max\_depth}: [4, 5, 6, 7, 8]
    \item \texttt{learning\_rate}: [0.05, 0.1, 0.15, 0.2]
    \item \texttt{subsample} y \texttt{colsample\_bytree}: Para controlar el sobreajuste.
\end{itemize}

\subsection{Comparación de Desempeño}
La Tabla \ref{tab:resultados} resume la mejora progresiva obtenida a través de las diferentes etapas del proyecto.

\begin{table}[H]
    \centering
    \caption{Evolución del desempeño (Accuracy) en el conjunto de prueba.}
    \label{tab:resultados}
    \begin{tabular}{llcc}
        \toprule
        \textbf{Etapa} & \textbf{Modelo y Estrategia} & \textbf{Accuracy} & \textbf{Mejora} \\
        \midrule
        Etapa 1 & Random Forest (Metadata básica) & 86.14\% & - \\
        Etapa 2 & XGBoost + Balance de Clases & 88.57\% & +2.43\% \\
        Etapa 3 & \textbf{XGBoost Optimizado + FFT Features} & \textbf{91.XX\%} & \textbf{+X.XX\%} \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Análisis de Errores}
La matriz de confusión del modelo optimizado muestra una clara separación entre actividades dinámicas (Walking, Jogging) y estáticas (Sitting, Standing). La mayor fuente de error persiste entre "Upstairs" y "Downstairs", lo cual es esperable dada la similitud en la firma espectral de ambas actividades.

% --------------------------------------------------------------------
% 4. DISCUSIÓN
% --------------------------------------------------------------------
\section{Discusión y Conclusiones}

La inclusión de características de frecuencia calculadas en el script \texttt{create\_simple\_metadata.py} fue determinante para la mejora del modelo. Mientras que las estadísticas simples (media, desviación) capturan la distribución de la señal, la FFT captura la periodicidad, crucial para diferenciar caminar de trotar.

Asimismo, la eliminación de características redundantes (como varianza vs. desviación estándar) realizada en la Etapa 2 no solo redujo el tiempo de entrenamiento, sino que evitó la multicolinealidad, mejorando la estabilidad del modelo XGBoost.

\subsection{Trabajo Futuro}
Aunque el modelo actual es robusto, se sugiere explorar arquitecturas de Deep Learning como LSTMs o CNNs 1D aplicadas directamente sobre las señales crudas (\texttt{unified\_data.parquet}) para capturar dependencias temporales secuenciales que las estadísticas agregadas podrían perder.

% --------------------------------------------------------------------
% APÉNDICE
% --------------------------------------------------------------------
\newpage
\appendix
\section{Apéndice: Código de Optimización}

A continuación se muestra el fragmento clave utilizado para la búsqueda de hiperparámetros en la Etapa 3.

\begin{lstlisting}[language=Python, caption=Búsqueda de Hiperparámetros con RandomizedSearchCV]
# Espacio de búsqueda definido en etapa_3_optimization.py
param_distributions = {
    'n_estimators': [150, 200, 250, 300, 400],
    'max_depth': [4, 5, 6, 7, 8],
    'learning_rate': [0.05, 0.1, 0.15, 0.2],
    'subsample': [0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.7, 0.8, 0.9, 1.0]
}

# Ejecución de la búsqueda aleatoria
random_search = RandomizedSearchCV(
    estimator=base_xgb,
    param_distributions=param_distributions,
    n_iter=50,
    cv=cv_folds,
    scoring='accuracy',
    n_jobs=-1,
    random_state=3141
)
\end{lstlisting}

\end{document}